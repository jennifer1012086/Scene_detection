{"cells":[{"cell_type":"code","source":["# 我會盡量幫大家做足註解\n","# 讓大家比較知道在做甚麼\n","# 基本上中文的註解應該是我註的\n","# 英文的就是原作者啦"],"metadata":{"id":"K7bEEtf5IFS-","executionInfo":{"status":"ok","timestamp":1653701885232,"user_tz":-480,"elapsed":1354,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# 目前已經測試過可以跑起來，再跑起來之前要請大家注意以下事項： \n","#       p.s. 記得設定捷徑連到 data 和 model_final 資料夾\n","# 1. \"model.compile()\" 那裏的 epoch 數字請根據自己做實驗需求設定，原作者設定 150\n","# 2. \"train_path\", \"test_path\" 是需要大家根據自己的資料夾位置填寫的變數，train 大約 2 處、test 大約 1 處，有特別註解強調，請大家再注意\n","# 3. \"save_model_path\" 同上，大約有 2 處，也請大家填上自己的 model_final 資料夾路徑\n","# 4. 執行 test 後，結果會存在 \"test_output.txt\"，但請大家把這個檔名改掉（不要跟別人重複），不然會不小心覆寫別人的檔案\n","# 5.  \"encoder_model.h5\", \"decoder_model_weights.h5\", \"tokenizer\" 更重要，一定要改檔名，不然覆寫掉了別人的會很恐怖\n","#       也就是說，只要 \"前綴有 model_final 資料夾的\"，請務必取一個別的檔名！！\n","# 6. 我發現執行 1 個 epoch 需要 1hr.，所以要留給他多點時間\n","\n","# 2022.5.5 更新 ##\n","# 1. batch_size 已經改成 256，有 1 處 model.fit 和 2 處 load_dataset 需要更改\n","# 2. 新增 2 個 block：安裝 SPICE 套件、執行 spice 計算 test 準確度（指定路徑和檔名也要改）\n","# 3. 補充：load_model 部分的 h5 檔案也需要更改\n","\n","# 2022.5.12 更新 ##\n","# 1. 新增 predict_realtime（無法播放影片，只能產生字幕）\n","# 2. 待測影片請放置新增資料夾 data/demo_data/video\n","# 3. 下面 config 部分，\"#######\" 註解那裏請改成自己的路徑及參數"],"metadata":{"id":"ZWBWiQ8J7rYB","executionInfo":{"status":"ok","timestamp":1653701885677,"user_tz":-480,"elapsed":3,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["### 連結雲端 ###\n","\n","# 可以在自己雲端建立資料集的捷徑就可以用了\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJwwMoxM_IMC","outputId":"9d51602f-c76f-4fc5-c69e-575f8000617c","executionInfo":{"status":"ok","timestamp":1653701913038,"user_tz":-480,"elapsed":27364,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"sg1KOmZE9rPG","executionInfo":{"status":"ok","timestamp":1653701913640,"user_tz":-480,"elapsed":606,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}}},"outputs":[],"source":["### 程式開跑前先import ###\n","import datetime\n","import os, sys\n","import json\n","import random\n","import numpy as np\n","import joblib\n","import keras\n","import tensorflow as tf\n","import pickle, functools, operator\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from keras.models import Model, load_model\n","from keras.layers import Input, LSTM, Dense\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.utils.vis_utils import plot_model"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_KCDV5hs-WbU","outputId":"07c7cec6-2db2-4854-9d47-44a8a9296d1f","executionInfo":{"status":"ok","timestamp":1653702030518,"user_tz":-480,"elapsed":116880,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["70823\n","10301\n","60199\n","10624\n"]},{"output_type":"execute_result","data":{"text/plain":["1870"]},"metadata":{},"execution_count":5}],"source":["### 把 training_data 全部 load 進來 ###\n","\n","# 設定 training data label 的 json 檔路徑\n","train_path='/content/drive/MyDrive/特徵擷取區bykelly/data/training_data'   ####### 這裡要改成自己的 training_data 資料夾（/content 之後） ######\n","TRAIN_LABEL_PATH = os.path.join(train_path, 'training_label.json')\n","\n","# 所有 training_data 還會再被分為 85% 的 training set & 15% 的 validation set\n","# mentioning the train test split\n","train_split = 0.85\n","\n","# load training data label 的 json 檔\n","# loading the json file for training\n","with open(TRAIN_LABEL_PATH) as data_file:    \n","    y_data = json.load(data_file)\n","\n","# train_list contains all the captions with their video ID\n","# vocab_list contains all the vocabulary from training data\n","train_list = []\n","vocab_list = []\n","\n","# 遍歷 json 檔案，在各句子前後分別加上 <bos>, <eos>，然後淘汰長度小於 6 or 大於 10 的句子\n","for y in y_data:\n","  for caption in y['caption']:\n","    caption = \"<bos> \" + caption + \" <eos>\"\n","    # we are only using sentences whose length lie between 6 and 10\n","    if len(caption.split())>14 or len(caption.split())<6:\n","      continue\n","    else:\n","      train_list.append([caption, y['id']])\n","print(len(train_list))\n","\n","# 把全部句子洗牌，然後分出 85% train 和 15% validation\n","random.shuffle(train_list)\n","training_list = train_list[:int(len(train_list)*train_split)]\n","validation_list = train_list[int(len(train_list)*train_split):]\n","\n","# 把每句 caption 加入 vocab_list，等下要用工具拆出單字\n","for train in training_list:\n","    vocab_list.append(train[0])\n","\n","# 用 Tokenizer 拆解每句 caption 的單字，並且取其中頻率最高的 1500 個單字 (Tokenizer 詳細用法解說：https://www.twblogs.net/a/5c113708bd9eee5e4183a621)\n","# Tokenizing the words\n","tokenizer = Tokenizer(num_words=3000)\n","tokenizer.fit_on_texts(vocab_list)\n","print(len(tokenizer.word_index))\n","\n","# 設定存 feature 的路徑\n","TRAIN_FEATURE_DIR = os.path.join(train_path, 'feat')\n","\n","# load 進來已經處理好的 training_data 的 feature\n","# Loading all the numpy arrays at once and saving them in a dictionary\n","x_data = {}\n","for filename in os.listdir(TRAIN_FEATURE_DIR):\n","    f = np.load(os.path.join(TRAIN_FEATURE_DIR, filename))\n","    x_data[filename[:-4] + \".avi\"] = f   # 把副檔名(.npy)去掉，只留 id\n","print(len(training_list))\n","print(len(validation_list))\n","len(x_data)\n","\n","# 需要有 training_data 的資料夾\n","# training_data 資料夾裡面要有 training_label.json\n","# training_data 資料夾裡面要有 feat 資料夾\n","# 這裡的 os 要改成可以讀的路徑"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"U82Pabyx-tBY","executionInfo":{"status":"ok","timestamp":1653702030519,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}}},"outputs":[],"source":["### Dataset 產生器 ###\n","# 分成 training set 和 validation set 讀進來，將同影片不同句子配對\n","\n","# 設定 train_data 資料夾路徑\n","train_path='/content/drive/MyDrive/特徵擷取區bykelly/data/training_data'   ####### 這裡要改成自己的 training_data 資料夾（/content 之後） ######\n","\n","# Creating a custom data generator because we cannot load so many files at once\n","def load_datatest(train_path, epochs=100, x_data=x_data, tokenizer=tokenizer, num_decoder_tokens=3000,training_list=train_list, batch_size=32, maxlen=14):\n","    encoder_input_data = []\n","    decoder_input_data = []\n","    decoder_target_data = []\n","    videoId = []\n","    videoSeq = []\n","\n","    # 拆成 ID, sequence 兩個 list，其中 training_list 的資料結構：(caption, video_id)\n","    # separating the videoId and the video captions\n","    for idx, cap in enumerate(training_list):\n","        caption = cap[0]\n","        videoId.append(cap[1])\n","        videoSeq.append(caption)\n","        \n","    # 把所有 caption 的單字全部轉成 np_array，長度不足 10 的補上 0\n","    # converting the captions to tokens and padding them to equal sizes\n","    train_sequences = tokenizer.texts_to_sequences(videoSeq)\n","    train_sequences = np.array(train_sequences)\n","    train_sequences = pad_sequences(train_sequences, padding='post',truncating='post', maxlen=maxlen)   # pad_sequences 的詳細用法解說：https://blog.csdn.net/wcy23580/article/details/84957471\n","    max_seq_length = train_sequences.shape[1]   # 都已經讓長度是 10 了所以這裡就是 10\n","    filesize = len(train_sequences)     # 總共有幾句 caption\n","\n","    # 組織 encoder, decoder 需要的資料結構\n","    X_data = []\n","    y_data = []\n","    vCount = 0\n","    n = 0\n","    for i in range(epochs):             # 要產生幾輪資料\n","      for idx in  range(0,filesize):    # 每句 caption\n","        n += 1\n","        encoder_input_data.append(x_data[videoId[idx]])                 # 存進 feature\n","        y = to_categorical(train_sequences[idx], num_decoder_tokens)    # 把每句 caption 的單字都照著 top 1500 單字下去編碼，沒在 top 1500 的字全會被分到同一類\n","        decoder_input_data.append(y[:-1])   # 去掉 <eos>\n","        decoder_target_data.append(y[1:])   # 去掉 <bos>\n","        if n == batch_size:     # 依照 batch_size 的大小分出資料\n","          encoder_input = np.array(encoder_input_data)\n","          decoder_input = np.array(decoder_input_data)\n","          decoder_target = np.array(decoder_target_data)\n","          encoder_input_data = []\n","          decoder_input_data = []\n","          decoder_target_data = []\n","          n = 0\n","          yield ([encoder_input, decoder_input], decoder_target)    # yield 會記住這次執行到哪裡，然後暫停住，把值 return 回去\n","                                                                    # yield 詳細用法解說：https://blog.csdn.net/mieleizhi0522/article/details/82142856"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"8gOjWCig-wub","executionInfo":{"status":"ok","timestamp":1653702030519,"user_tz":-480,"elapsed":4,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}}},"outputs":[],"source":["### 宣告 train set 以及 validation set 的生成器 ###\n","# 因為 load_data 用了 yield，已經不是一個函式了，可以當作一個生成器，這裡只是宣告生成器而已\n","# writing the train and validation generator\n","train = load_datatest(train_path=train_path,batch_size=256, training_list=training_list, x_data=x_data, epochs=150)\n","valid = load_datatest(train_path=train_path,batch_size=256, training_list=validation_list, x_data=x_data, epochs=150)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZACEJzuP-2ji","executionInfo":{"status":"ok","timestamp":1653702030519,"user_tz":-480,"elapsed":4,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}}},"outputs":[],"source":["### 宣告會用到的常數 ###\n","\"\"\"\n","time_steps_encoder is the number of frames per video we will be using for training\n","num_encoder_tokens is the number of features from each frame\n","latent_dim is the number of hidden features for lstm\n","time_steps_decoder is the maximum length of each sentence\n","num_decoder_tokens is the final number of tokens in the softmax layer\n","batch size\n","\"\"\"\n","time_steps_encoder=80       # frame 數量\n","num_encoder_tokens=4096     # 每個 frame 的特徵數量\n","latent_dim=512              # LSTM 的 hidden features\n","time_steps_decoder=8       # 每個產生的句子的最大長度\n","num_decoder_tokens=3000     # 產生的單字選擇數\n","batch_size=256              # batch size"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"YTORZjeFGLuI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"886b6035-0aac-4d38-ef1b-24db23f8b179","executionInfo":{"status":"ok","timestamp":1653702035621,"user_tz":-480,"elapsed":5106,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_inputs (InputLayer)    [(None, 80, 4096)]   0           []                               \n","                                                                                                  \n"," decoder_inputs (InputLayer)    [(None, 8, 3000)]    0           []                               \n","                                                                                                  \n"," endcoder_lstm (LSTM)           [(None, 80, 512),    9439232     ['encoder_inputs[0][0]']         \n","                                 (None, 512),                                                     \n","                                 (None, 512)]                                                     \n","                                                                                                  \n"," decoder_lstm (LSTM)            [(None, 8, 512),     7194624     ['decoder_inputs[0][0]',         \n","                                 (None, 512),                     'endcoder_lstm[0][1]',          \n","                                 (None, 512)]                     'endcoder_lstm[0][2]']          \n","                                                                                                  \n"," decoder_relu (Dense)           (None, 8, 3000)      1539000     ['decoder_lstm[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 18,172,856\n","Trainable params: 18,172,856\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["### 宣告 Encoder 和 Decoder ###\n","\n","# 他的 encoder, decoder 設計方式和官網範例一樣：https://keras.io/zh/examples/lstm_seq2seq/\n","# Setting up the encoder\n","encoder_inputs = Input(shape=(time_steps_encoder, num_encoder_tokens), name=\"encoder_inputs\")   # Input 層，(80, 4096)\n","encoder = LSTM(latent_dim, return_state=True,return_sequences=True, name='endcoder_lstm')       # LSTM\n","_, state_h, state_c = encoder(encoder_inputs)\n","encoder_states = [state_h, state_c]\n","\n","\n","# Set up the decoder\n","decoder_inputs = Input(shape=(time_steps_decoder, num_decoder_tokens), name= \"decoder_inputs\")  # Input 層，(80, 10)\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')  # LSTM\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_relu')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.summary()\n","# plot_model(model, to_file='model_train.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"UuaCM61eGmMp","executionInfo":{"status":"ok","timestamp":1653702035621,"user_tz":-480,"elapsed":5,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}}},"outputs":[],"source":["### 不一定要執行，看訓練過程的 ###\n","# Loading the tensorboard to visualise results\n","#%load_ext tensorboard\n","#import datetime\n","#%tensorboard --logdir logs"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"28unBh23G0mL","executionInfo":{"status":"ok","timestamp":1653706021234,"user_tz":-480,"elapsed":3985618,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"620dd85e-99a7-4bf5-c001-697e485c86ed"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/28\n","235/235 [==============================] - 143s 578ms/step - loss: 2.9297 - accuracy: 0.5269 - val_loss: 2.4886 - val_accuracy: 0.5599 - lr: 3.5000e-04\n","Epoch 2/28\n","235/235 [==============================] - 143s 610ms/step - loss: 2.3292 - accuracy: 0.5955 - val_loss: 2.1588 - val_accuracy: 0.6279 - lr: 3.5000e-04\n","Epoch 3/28\n","235/235 [==============================] - 143s 609ms/step - loss: 2.0399 - accuracy: 0.6422 - val_loss: 1.9144 - val_accuracy: 0.6584 - lr: 3.5000e-04\n","Epoch 4/28\n","235/235 [==============================] - 143s 608ms/step - loss: 1.8171 - accuracy: 0.6709 - val_loss: 1.7295 - val_accuracy: 0.6815 - lr: 3.5000e-04\n","Epoch 5/28\n","235/235 [==============================] - 135s 575ms/step - loss: 1.6507 - accuracy: 0.6929 - val_loss: 1.5979 - val_accuracy: 0.6987 - lr: 3.5000e-04\n","Epoch 6/28\n","235/235 [==============================] - 135s 575ms/step - loss: 1.5237 - accuracy: 0.7092 - val_loss: 1.4909 - val_accuracy: 0.7139 - lr: 3.5000e-04\n","Epoch 7/28\n","235/235 [==============================] - 144s 612ms/step - loss: 1.4231 - accuracy: 0.7221 - val_loss: 1.4129 - val_accuracy: 0.7245 - lr: 3.5000e-04\n","Epoch 8/28\n","235/235 [==============================] - 144s 611ms/step - loss: 1.3429 - accuracy: 0.7326 - val_loss: 1.3519 - val_accuracy: 0.7322 - lr: 3.5000e-04\n","Epoch 9/28\n","235/235 [==============================] - 143s 610ms/step - loss: 1.2787 - accuracy: 0.7405 - val_loss: 1.3016 - val_accuracy: 0.7385 - lr: 3.5000e-04\n","Epoch 10/28\n","235/235 [==============================] - 143s 611ms/step - loss: 1.2255 - accuracy: 0.7467 - val_loss: 1.2654 - val_accuracy: 0.7424 - lr: 3.5000e-04\n","Epoch 11/28\n","235/235 [==============================] - 143s 609ms/step - loss: 1.1809 - accuracy: 0.7518 - val_loss: 1.2313 - val_accuracy: 0.7467 - lr: 3.5000e-04\n","Epoch 12/28\n","235/235 [==============================] - 136s 577ms/step - loss: 1.1439 - accuracy: 0.7560 - val_loss: 1.2062 - val_accuracy: 0.7499 - lr: 3.5000e-04\n","Epoch 13/28\n","235/235 [==============================] - 136s 579ms/step - loss: 1.1110 - accuracy: 0.7598 - val_loss: 1.1853 - val_accuracy: 0.7524 - lr: 3.5000e-04\n","Epoch 14/28\n","235/235 [==============================] - 143s 608ms/step - loss: 1.0832 - accuracy: 0.7631 - val_loss: 1.1650 - val_accuracy: 0.7552 - lr: 3.5000e-04\n","Epoch 15/28\n","235/235 [==============================] - 144s 612ms/step - loss: 1.0586 - accuracy: 0.7659 - val_loss: 1.1530 - val_accuracy: 0.7560 - lr: 3.5000e-04\n","Epoch 16/28\n","235/235 [==============================] - 136s 581ms/step - loss: 1.0364 - accuracy: 0.7683 - val_loss: 1.1375 - val_accuracy: 0.7576 - lr: 3.5000e-04\n","Epoch 17/28\n","235/235 [==============================] - 136s 579ms/step - loss: 1.0168 - accuracy: 0.7704 - val_loss: 1.1287 - val_accuracy: 0.7582 - lr: 3.5000e-04\n","Epoch 18/28\n","235/235 [==============================] - 143s 607ms/step - loss: 0.9990 - accuracy: 0.7726 - val_loss: 1.1166 - val_accuracy: 0.7597 - lr: 3.5000e-04\n","Epoch 19/28\n","235/235 [==============================] - 143s 609ms/step - loss: 0.9835 - accuracy: 0.7744 - val_loss: 1.1105 - val_accuracy: 0.7603 - lr: 3.5000e-04\n","Epoch 20/28\n","235/235 [==============================] - 143s 609ms/step - loss: 0.9679 - accuracy: 0.7762 - val_loss: 1.1030 - val_accuracy: 0.7619 - lr: 3.5000e-04\n","Epoch 21/28\n","235/235 [==============================] - 143s 609ms/step - loss: 0.9546 - accuracy: 0.7779 - val_loss: 1.0942 - val_accuracy: 0.7631 - lr: 3.5000e-04\n","Epoch 22/28\n","235/235 [==============================] - 143s 610ms/step - loss: 0.9405 - accuracy: 0.7795 - val_loss: 1.0914 - val_accuracy: 0.7634 - lr: 3.5000e-04\n","Epoch 23/28\n","235/235 [==============================] - 136s 581ms/step - loss: 0.9278 - accuracy: 0.7811 - val_loss: 1.0850 - val_accuracy: 0.7641 - lr: 3.5000e-04\n","Epoch 24/28\n","235/235 [==============================] - 143s 610ms/step - loss: 0.9163 - accuracy: 0.7826 - val_loss: 1.0828 - val_accuracy: 0.7642 - lr: 3.5000e-04\n","Epoch 25/28\n","235/235 [==============================] - 137s 582ms/step - loss: 0.9054 - accuracy: 0.7838 - val_loss: 1.0783 - val_accuracy: 0.7651 - lr: 3.5000e-04\n","Epoch 26/28\n","235/235 [==============================] - 144s 611ms/step - loss: 0.8948 - accuracy: 0.7853 - val_loss: 1.0759 - val_accuracy: 0.7651 - lr: 3.5000e-04\n","Epoch 27/28\n","235/235 [==============================] - 143s 610ms/step - loss: 0.8861 - accuracy: 0.7862 - val_loss: 1.0739 - val_accuracy: 0.7646 - lr: 3.5000e-04\n","Epoch 28/28\n","235/235 [==============================] - 136s 581ms/step - loss: 0.8777 - accuracy: 0.7872 - val_loss: 1.0687 - val_accuracy: 0.7654 - lr: 3.5000e-04\n"]}],"source":["### 設定一些停止條件以後就可以讓 model 跑起來了 ###\n","\n","# 一旦 validation set 連續 5 個 epoch 都沒有再提升，就停止訓練（詳細用法解說：https://cynthiachuang.github.io/EarlyStopping-Callback/）\n","# Early Stopping\n","earlystopping = EarlyStopping(monitor='val_loss', patience = 5, verbose=1, mode='min')\n","\n","# Tensorboard callback\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","# 設定優化方式，開跑！\n","# Run training\n","opt = tf.keras.optimizers.Adam(lr = 0.00035)\n","x = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,patience=2,verbose=0,mode=\"auto\")\n","model.compile(metrics=['accuracy'], optimizer=opt, loss='categorical_crossentropy')\n","\n","\n","try:\n","    model.fit(train, validation_data=valid, validation_steps=(len(validation_list)//batch_size),\n","        epochs=28, steps_per_epoch=(len(training_list)//batch_size),\n","            callbacks=[x, earlystopping, tensorboard_callback])\n","except KeyboardInterrupt:\n","    print(\"\\nW: interrupt received, stopping\")\n","finally:\n","    pass"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"FqMRfxX3HjhT","colab":{"base_uri":"https://localhost:8080/","height":266},"executionInfo":{"status":"ok","timestamp":1653706021235,"user_tz":-480,"elapsed":17,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}},"outputId":"b689050f-92a8-4cd4-9457-e2d1bba2be50"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["### 不一定要執行，看訓練結果的 ###\n","import matplotlib.pyplot as plt\n","plt.plot(model.history.history['loss'])\n","plt.plot(model.history.history['val_loss'])\n","plt.legend(['train', 'test'], loc='upper right')\n","plt.savefig('loss.png')\n","plt.show()"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"pVhMN3uCILGe","colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"status":"ok","timestamp":1653706022058,"user_tz":-480,"elapsed":13,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}},"outputId":"ceadaf5d-df19-4851-d4ae-471c99a171b1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["### 不一定要執行，看訓練結果的 ###\n","plt.plot(model.history.history['accuracy'])\n","plt.plot(model.history.history['val_accuracy'])\n","plt.legend(['train', 'test'], loc='upper right')\n","plt.savefig('accuracy.png')\n","plt.show()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"6NVfxmnTZkzF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653706029693,"user_tz":-480,"elapsed":7647,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}},"outputId":"02dcd994-371b-4dc1-8738-668f4446d575"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," encoder_inputs (InputLayer)  [(None, 80, 4096)]       0         \n","                                                                 \n"," endcoder_lstm (LSTM)        [(None, 80, 512),         9439232   \n","                              (None, 512),                       \n","                              (None, 512)]                       \n","                                                                 \n","=================================================================\n","Total params: 9,439,232\n","Trainable params: 9,439,232\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," decoder_inputs (InputLayer)    [(None, 8, 3000)]    0           []                               \n","                                                                                                  \n"," input_1 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 512)]        0           []                               \n","                                                                                                  \n"," decoder_lstm (LSTM)            [(None, 8, 512),     7194624     ['decoder_inputs[0][0]',         \n","                                 (None, 512),                     'input_1[0][0]',                \n","                                 (None, 512)]                     'input_2[0][0]']                \n","                                                                                                  \n"," decoder_relu (Dense)           (None, 8, 3000)      1539000     ['decoder_lstm[1][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 8,733,624\n","Trainable params: 8,733,624\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}],"source":["### 把 train 好的 model 存起來 ###\n","\n","# 建立 model_final 資料夾       ####### 記得改成自己的 \"model_final\" 路徑 #######\n","save_model_path = '/content/drive/MyDrive/ColabNotebooks/model_final'\n","if not os.path.exists(save_model_path):\n","    os.makedirs(save_model_path)\n","\n","# Saving encoder as in training\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# Saving decoder states and dense layer \n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)\n","\n","# 印出 model 總結的，不用太在意\n","encoder_model.summary()\n","decoder_model.summary()\n","\n","# 把 Encoder 和 Decoder 各存成 h5 檔，也寫入 tokenizer1500 單字檔\n","####### \"encoder_model.h5\", \"decoder_model_weights.h5\", \"tokenizer\" 3 個檔案也要改檔名！！ #######\n","encoder_model.save(os.path.join(save_model_path, 'jennifer_1_encoder_model.h5'))\n","decoder_model.save_weights(os.path.join(save_model_path, 'jennifer_1_decoder_model_weights.h5'))\n","with open(os.path.join(save_model_path,'jennifer_1_tokenizer'+ str(num_decoder_tokens) ),'wb') as file:\n","    joblib.dump(tokenizer, file)\n","\n","# plot_model(encoder_model, to_file='model_inference_encoder.png', show_shapes=True, show_layer_names=True)\n","# plot_model(decoder_model, to_file='model_inference_decoder.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"gn7acHsca7n3","executionInfo":{"status":"ok","timestamp":1653706030740,"user_tz":-480,"elapsed":1048,"user":{"displayName":"陳冠羽","userId":"13624323751069066354"}}},"outputs":[],"source":["### 這裡是定義 test 的部分 ###\n","# test 執行後會存成一個 txt 檔案再 model_final 資料夾，可以再去那邊確認\n","# class to perform inference on all test files and save as test_output.txt\n","class Video2Text(object):\n","    ''' Initialize the parameters for the model '''\n","    def __init__(self):\n","        self.latent_dim = 512\n","        self.num_encoder_tokens = 4096\n","        self.num_decoder_tokens = 3000\n","        self.time_steps_encoder = 80\n","        self.time_steps_decoder = None\n","        self.preload = True\n","        self.preload_data_path = 'preload_data'\n","        self.max_probability = -1\n","\n","        # processed data\n","        self.encoder_input_data = []\n","        self.decoder_input_data = []\n","        self.decoder_target_data = []\n","        self.tokenizer = None\n","\n","        # models\n","        self.encoder_model = None\n","        self.decoder_model = None\n","        self.inf_encoder_model = None\n","        self.inf_decoder_model = None       ####### 下面 2 行要記得改成自己的 \"model_final\" 和 \"testing_data\" 路徑 #######\n","        self.save_model_path = '/content/drive/MyDrive/ColabNotebooks/model_final'\n","        self.test_path = '/content/drive/MyDrive/特徵擷取區bykelly/data/testing_data'\n","        \n","    def load_inference_models(self):\n","        # load tokenizer\n","        \n","        with open(os.path.join(self.save_model_path, 'jennifer_1_tokenizer' + str(self.num_decoder_tokens)), 'rb') as file:\n","            self.tokenizer = joblib.load(file)\n","\n","        # inference encoder model\n","        self.inf_encoder_model = load_model(os.path.join(self.save_model_path, 'jennifer_1_encoder_model.h5'))\n","\n","        # inference decoder model\n","        decoder_inputs = Input(shape=(None, self.num_decoder_tokens))\n","        decoder_dense = Dense(self.num_decoder_tokens, activation='softmax')\n","        decoder_lstm = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n","        decoder_state_input_h = Input(shape=(self.latent_dim,))\n","        decoder_state_input_c = Input(shape=(self.latent_dim,))\n","        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","        decoder_states = [state_h, state_c]\n","        decoder_outputs = decoder_dense(decoder_outputs)\n","        self.inf_decoder_model = Model(\n","            [decoder_inputs] + decoder_states_inputs,\n","            [decoder_outputs] + decoder_states)\n","        self.inf_decoder_model.load_weights(os.path.join(self.save_model_path, 'jennifer_1_decoder_model_weights.h5'))\n","    \n","    def decode_sequence2bs(self, input_seq):\n","        states_value = self.inf_encoder_model.predict(input_seq)\n","        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n","        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n","        self.beam_search(target_seq, states_value,[],[],0)\n","        return decode_seq\n","\n","    def beam_search(self, target_seq, states_value, prob,  path, lens):\n","        global decode_seq\n","        node = 2\n","        output_tokens, h, c = self.inf_decoder_model.predict(\n","            [target_seq] + states_value)\n","        output_tokens = output_tokens.reshape((self.num_decoder_tokens))\n","        sampled_token_index = output_tokens.argsort()[-node:][::-1]\n","        states_value = [h, c]\n","        for i in range(node):\n","            if sampled_token_index[i] == 0:\n","                sampled_char = ''\n","            else:\n","                sampled_char = list(self.tokenizer.word_index.keys())[list(self.tokenizer.word_index.values()).index(sampled_token_index[i])]\n","            MAX_LEN = 14\n","            if(sampled_char != 'eos' and lens <= MAX_LEN):\n","                p = output_tokens[sampled_token_index[i]]\n","                if(sampled_char == ''):\n","                    p = 1\n","                prob_new = list(prob)\n","                prob_new.append(p)\n","                path_new = list(path)\n","                path_new.append(sampled_char)\n","                target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n","                target_seq[0, 0, sampled_token_index[i]] = 1.\n","                self.beam_search(target_seq, states_value, prob_new, path_new, lens+1)\n","            else:\n","                p = output_tokens[sampled_token_index[i]]\n","                prob_new = list(prob)\n","                prob_new.append(p)\n","                p = functools.reduce(operator.mul, prob_new, 1)\n","                if(p > self.max_probability):\n","                    decode_seq = path\n","                    self.max_probability = p\n","\n","    def decoded_sentence_tuning(self, decoded_sentence):\n","        decode_str = []\n","        filter_string = ['bos', 'eos']\n","        unigram = {}\n","        last_string = \"\"\n","        for idx2, c in enumerate(decoded_sentence):\n","            if c in unigram:\n","                unigram[c] += 1\n","            else:\n","                unigram[c] = 1\n","            if(last_string == c and idx2 > 0):\n","                continue\n","            if c in filter_string:\n","                continue\n","            if len(c) > 0:\n","                decode_str.append(c)\n","            if idx2 > 0:\n","                last_string = c\n","        return decode_str\n","\n","    def get_test_data(self, path):\n","        X_test = []\n","        X_test_filename = []\n","        with open (os.path.join(path, 'testing_id.txt')) as testing_file:\n","            lines = testing_file.readlines()\n","            for filename in lines:\n","                filename = filename.strip()\n","                f = np.load(os.path.join(path , 'feat', filename[:-4] + '.npy'))\n","                X_test.append(f)\n","                X_test_filename.append(filename[:-4])\n","            X_test = np.array(X_test)\n","        return X_test, X_test_filename\n","\n","    def test(self):\n","        X_test, X_test_filename = self.get_test_data(os.path.join(self.test_path))\n","        print(len(X_test), len(X_test_filename))\n","        # generate inference test outputs     ####### test_output.txt 請改成自己取的檔名，才不會把別人的覆寫掉 #######\n","        with open(os.path.join(self.save_model_path, 'jennifer_1_test_output.txt'), 'w') as file:\n","            for idx, x in enumerate(X_test): \n","                file.write(X_test_filename[idx]+',')\n","                decoded_sentence = self.decode_sequence2bs(x.reshape(-1, 80, 4096))\n","                decode_str = self.decoded_sentence_tuning(decoded_sentence)\n","                for d in decode_str:\n","                    file.write(d + ' ')\n","                file.write('\\n')\n","                # re-init max prob\n","                self.max_probability = -1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Es2Lu-Qa-kw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4311c4f5-b1d0-4466-f756-49b45d1f0280"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","100 100\n"]}],"source":["### 執行 test，結果去 model_final 的 txt 檔案確認 ###\n","# 印出的數字表示 test 的影片總共有幾個\n","c = Video2Text()\n","c.load_inference_models()\n","c.test()"]},{"cell_type":"code","source":["### 安裝 SPICE 需要的套件 ###\n","!pip install \"git+https://github.com/salaniz/pycocoevalcap.git\""],"metadata":{"id":"FYlvxIcFVMjC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 計算 test 準確度 ###\n","import os, json\n","from pycocoevalcap.spice.spice import Spice\n","\n","# 設定路徑      ####### 改成相對應的路徑 #######\n","savePath = \"/content/drive/MyDrive/ColabNotebooks/model_final\"\n","testPath = \"/content/drive/MyDrive/特徵擷取區bykelly/data/testing_data\"\n","\n","# 處理 predict      ####### 改成自己的 test_output 檔名 ######\n","prdt = open(os.path.join(savePath , \"jennifer_1_test_output.txt\"), \"r\")      \n","prdtObj = {}\n","for sentence in prdt:\n","    sentence = sentence.split(\",\", 1)\n","    id, caption = sentence[0].strip(), [ sentence[1].strip() ]\n","    prdtObj[id] = caption\n","prdt.close()\n","# print(res)\n","\n","# 讀取 json 檔案，處理格式\n","labelFile = open(os.path.join(testPath , \"testing_label.json\"), \"r\")\n","labelJson = labelFile.read()\n","label = json.loads(labelJson)\n","lblObj = {}\n","for obj in label:\n","    id, captions = obj[\"id\"][:-4], obj[\"caption\"]\n","    lblObj[id] = captions\n","labelFile.close()\n","# print((gts))\n","\n","# 用 spice 計算\n","scorer = Spice()\n","score, scores = scorer.compute_score(lblObj, prdtObj)  # lblObj(gts): label 答案\n","print('spice score = %s' % score)                      # prdtObj(res): predict 結果"],"metadata":{"id":"5jb0C8cGVMg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 設定 config 需要的參數 ###\n","test_path = \"/content/drive/MyDrive/特徵擷取區bykelly/data/demo_data\"     #######\n","latent_dim = 512\n","num_encoder_tokens = 4096\n","num_decoder_tokens = 3000                           #######\n","time_steps_encoder = 80\n","max_probability = -1\n","save_model_path = \"/content/drive/MyDrive/ColabNotebooks/model_final\"  #######\n","encoder_file = \"jennifer_1_encoder_model.h5\"                   #######\n","decoder_file = \"jennifer_1_decoder_model_weights.h5\"           #######\n","validation_split = 0.15\n","max_length = 14 \\                                    #######\n","search_type = 'greedy'"],"metadata":{"id":"zG8sVvvVEcU8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 影片需要先被特徵擷取 ###\n","import tqdm\n","import numpy as np\n","import cv2\n","import os\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.models import Model\n","\n","def video_to_frames(video):\n","    video_path = os.path.join(test_path, 'video', video)\n","    count = 0\n","    image_list = []\n","    # Path to video file\n","    cap = cv2.VideoCapture(video_path)\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if ret is False:\n","            break\n","        image_list.append(frame)\n","        count += 1\n","\n","    cap.release()\n","    cv2.destroyAllWindows()\n","    return image_list\n","\n","\n","def model_cnn_load():   ####### ResNet 要弄對 model #######\n","    model = VGG16(weights=\"imagenet\", include_top=True, input_shape=(224, 224, 3))\n","    # model = ResNet50(weights=\"imagenet\", include_top=True, input_shape=(224, 224, 3))\n","    out = model.layers[-2].output\n","    model_final = Model(inputs=model.input, outputs=out)\n","    return model_final\n","\n","\n","def load_image(oriImg):\n","    img = cv2.resize(oriImg, (224, 224))\n","    return img\n","\n","\n","def extract_features(video, model):\n","    \"\"\"\n","\n","    :param video: The video whose frames are to be extracted to convert into a numpy array\n","    :param model: the pretrained vgg16 model\n","    :return: numpy array of size 4096x80\n","    \"\"\"\n","    video_id = video.split(\".\")[0]\n","    print(video_id)\n","    print(f'Processing video {video}')\n","\n","    image_list = video_to_frames(video)\n","    samples = np.round( np.linspace(0, len(image_list) - 1, 80) )\n","    image_list = [image_list[int(sample)] for sample in samples]\n","    images = np.zeros((len(image_list), 224, 224, 3))\n","    for i in range(len(image_list)):\n","        img = load_image(image_list[i])\n","        images[i] = img\n","    images = np.array(images)\n","    fc_feats = model.predict(images, batch_size=128)\n","    img_feats = np.array(fc_feats)\n","    return img_feats"],"metadata":{"id":"1MXMuwa3EeEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 預測上傳要用的影片 ###\n","import functools\n","import operator\n","import os\n","import cv2\n","import time\n","\n","import joblib\n","import numpy as np\n","from keras.layers import Input, LSTM, Dense\n","from keras.models import Model, load_model\n","from google.colab.patches import cv2_imshow\n","\n","class VideoDescriptionRealTime(object):\n","    \"\"\"\n","        Initialize the parameters for the model\n","        \"\"\"\n","    def __init__(self):\n","        self.latent_dim = latent_dim\n","        self.num_encoder_tokens = num_encoder_tokens\n","        self.num_decoder_tokens = num_decoder_tokens\n","        self.time_steps_encoder = time_steps_encoder\n","        self.max_probability = max_probability\n","\n","        # models\n","        self.encoder_model = None\n","        self.decoder_model = None\n","        self.inf_encoder_model = None\n","        self.inf_decoder_model = None\n","        self.save_model_path = save_model_path\n","        self.encoder_file = encoder_file\n","        self.decoder_file = decoder_file\n","        self.test_path = test_path\n","        self.search_type = search_type\n","        self.tokenizer = None\n","        self.num = 0\n","\n","    def load_inference_models(self):\n","        # load tokenizer\n","\n","        with open(os.path.join(self.save_model_path, 'jennifer_1_tokenizer' + str(self.num_decoder_tokens)), 'rb') as file:\n","            self.tokenizer = joblib.load(file)\n","\n","        # inference encoder model\n","        self.inf_encoder_model = load_model(os.path.join(self.save_model_path, self.encoder_file))\n","\n","        # inference decoder model\n","        decoder_inputs = Input(shape=(None, self.num_decoder_tokens))\n","        decoder_dense = Dense(self.num_decoder_tokens, activation='softmax')\n","        decoder_lstm = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n","        decoder_state_input_h = Input(shape=(self.latent_dim,))\n","        decoder_state_input_c = Input(shape=(self.latent_dim,))\n","        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","        decoder_states = [state_h, state_c]\n","        decoder_outputs = decoder_dense(decoder_outputs)\n","        self.inf_decoder_model = Model(\n","            [decoder_inputs] + decoder_states_inputs,\n","            [decoder_outputs] + decoder_states)\n","        self.inf_decoder_model.load_weights(os.path.join(self.save_model_path, self.decoder_file))\n","\n","    def greedy_search(self, f):\n","        \"\"\"\n","\n","        :param f: the loaded numpy array after creating videos to frames and extracting features\n","        :return: the final sentence which has been predicted greedily\n","        \"\"\"\n","        inv_map = self.index_to_word()\n","        states_value = self.inf_encoder_model.predict(f.reshape(-1, 80, self.num_encoder_tokens))   ###\n","        target_seq = np.zeros((1, 1, 3000))\n","        final_sentence = ''\n","        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n","        for i in range(15):\n","            output_tokens, h, c = self.inf_decoder_model.predict([target_seq] + states_value)\n","            states_value = [h, c]\n","            output_tokens = output_tokens.reshape(self.num_decoder_tokens)\n","            y_hat = np.argmax(output_tokens)\n","            if y_hat == 0:\n","                continue\n","            if inv_map[y_hat] is None:\n","                break\n","            if inv_map[y_hat] == 'eos':\n","                break\n","            else:\n","                final_sentence = final_sentence + inv_map[y_hat] + ' '\n","                target_seq = np.zeros((1, 1, 3000))\n","                target_seq[0, 0, y_hat] = 1\n","        return final_sentence\n","\n","    def decode_sequence2bs(self, input_seq):\n","        states_value = self.inf_encoder_model.predict(input_seq)\n","        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n","        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n","        self.beam_search(target_seq, states_value, [], [], 0)\n","        return decode_seq\n","\n","    def beam_search(self, target_seq, states_value, prob, path, lens):\n","        \"\"\"\n","\n","        :param target_seq: the array that is fed into the model to predict the next word\n","        :param states_value: previous state that is fed into the lstm cell\n","        :param prob: probability of predicting a word\n","        :param path: list of words from each sentence\n","        :param lens: number of words\n","        :return: final sentence\n","        \"\"\"\n","        global decode_seq\n","        node = 2\n","        output_tokens, h, c = self.inf_decoder_model.predict(\n","            [target_seq] + states_value)\n","        output_tokens = output_tokens.reshape(self.num_decoder_tokens)\n","        sampled_token_index = output_tokens.argsort()[-node:][::-1]\n","        states_value = [h, c]\n","        for i in range(node):\n","            if sampled_token_index[i] == 0:\n","                sampled_char = ''\n","            else:\n","                sampled_char = list(self.tokenizer.word_index.keys())[\n","                    list(self.tokenizer.word_index.values()).index(sampled_token_index[i])]\n","            MAX_LEN = 14\n","            if sampled_char != 'eos' and lens <= MAX_LEN:\n","                p = output_tokens[sampled_token_index[i]]\n","                if sampled_char == '':\n","                    p = 1\n","                prob_new = list(prob)\n","                prob_new.append(p)\n","                path_new = list(path)\n","                path_new.append(sampled_char)\n","                target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n","                target_seq[0, 0, sampled_token_index[i]] = 1.\n","                self.beam_search(target_seq, states_value, prob_new, path_new, lens + 1)\n","            else:\n","                p = output_tokens[sampled_token_index[i]]\n","                prob_new = list(prob)\n","                prob_new.append(p)\n","                p = functools.reduce(operator.mul, prob_new, 1)\n","                if p > self.max_probability:\n","                    decode_seq = path\n","                    self.max_probability = p\n","\n","    def decoded_sentence_tuning(self, decoded_sentence):\n","        # tuning sentence\n","        decode_str = []\n","        filter_string = ['bos', 'eos']\n","        uni_gram = {}\n","        last_string = \"\"\n","        for idx2, c in enumerate(decoded_sentence):\n","            if c in uni_gram:\n","                uni_gram[c] += 1\n","            else:\n","                uni_gram[c] = 1\n","            if last_string == c and idx2 > 0:\n","                continue\n","            if c in filter_string:\n","                continue\n","            if len(c) > 0:\n","                decode_str.append(c)\n","            if idx2 > 0:\n","                last_string = c\n","        return decode_str\n","\n","    def index_to_word(self):\n","        # inverts word tokenizer\n","        index_to_word = {value: key for key, value in self.tokenizer.word_index.items()}\n","        return index_to_word\n","\n","    def get_test_data(self):\n","        # loads the features array\n","        file_list = os.listdir(os.path.join(self.test_path, 'video'))\n","        # with open(os.path.join(self.test_path, 'testing.txt')) as testing_file:\n","            # lines = testing_file.readlines()\n","        # file_name = lines[self.num].strip()\n","        file_name = file_list[self.num]\n","        path = os.path.join(self.test_path, 'feat', file_name + '.npy')\n","        if os.path.exists(path):\n","            f = np.load(path)\n","        else:\n","            model = model_cnn_load()\n","            f = extract_features(file_name, model)\n","        if self.num < len(file_list):\n","            self.num += 1\n","        else:\n","            self.num = 0\n","        return f, file_name\n","\n","    def test(self):\n","        X_test, filename = self.get_test_data()\n","        # generate inference test outputs\n","        if self.search_type == 'greedy':\n","            sentence_predicted = self.greedy_search(X_test.reshape((-1, 80, self.num_encoder_tokens)))      ###\n","        else:\n","            sentence_predicted = ''\n","            decoded_sentence = self.decode_sequence2bs(X_test.reshape((-1, 80, self.num_encoder_tokens)))   ###\n","            decode_str = self.decoded_sentence_tuning(decoded_sentence)\n","            for d in decode_str:\n","                sentence_predicted = sentence_predicted + d + ' '\n","        # re-init max prob\n","        self.max_probability = -1\n","        return sentence_predicted, filename\n","\n","    def main(self, filename, caption):\n","        \"\"\"\n","\n","        :param filename: the video to load\n","        :param caption: final caption\n","        :return:\n","        \"\"\"\n","        # 1. Initialize reading video object\n","        cap1 = cv2.VideoCapture(os.path.join(self.test_path, 'video', filename))\n","        cap2 = cv2.VideoCapture(os.path.join(self.test_path, 'video', filename))\n","        caption = '[' + ' '.join(caption.split()[1:]) + ']'\n","        # # 2. Cycle through pictures\n","        # while cap1.isOpened():\n","        #     ret, frame = cap2.read()\n","        #     ret2, frame2 = cap1.read()\n","        #     if ret:\n","        #         imS = cv2.resize(frame, (480, 300))\n","        #         cv2.putText(imS, caption, (100, 270), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0),\n","        #                     2, cv2.LINE_4)\n","        #         cv2_imshow(imS)\n","        #     if ret2:\n","        #         imS = cv2.resize(frame, (480, 300))\n","        #         cv2_imshow(imS)\n","        #     else:\n","        #         break\n","\n","        #     # Quit playing\n","        #     key = cv2.waitKey(25)\n","        #     if key == 27:  # Button esc\n","        #         break\n","\n","        # 3. Free resources\n","        cap1.release()\n","        cap2.release()\n","        cv2.destroyAllWindows()\n","\n","\n","if __name__ == \"__main__\":\n","    video_to_text = VideoDescriptionRealTime()\n","    video_to_text.load_inference_models()\n","    while True:\n","        print('.........................\\nGenerating Caption:\\n')\n","        start = time.time()\n","        video_caption, file = video_to_text.test()\n","        end = time.time()\n","        sentence = ''\n","        print(sentence)\n","        for text in video_caption.split():\n","            sentence = sentence + ' ' + text\n","            print('\\n.........................\\n')\n","            print(sentence)\n","        print('\\n.........................\\n')\n","        print('It took {:.2f} seconds to generate caption'.format(end-start))\n","        video_to_text.main(file, sentence)\n","        play_video = input('Should I play the video? ')\n","        if play_video.lower() == 'y':\n","            continue\n","        elif play_video.lower() == 'n':\n","            break\n","        else:\n","            print('Could not understand type (y) for yes and (n) for no')\n","            continue"],"metadata":{"id":"KMIYOBc4EfsP"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"J_Video_Captioning_2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}